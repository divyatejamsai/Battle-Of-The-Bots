{"cells":[{"cell_type":"markdown","metadata":{"id":"K_ymh-5LLyMY"},"source":["## Assignment\n","* Load and preprocess CIFAR100 dataset (not CIFAR10)\n","* Build a feedforward network for it. You can experiment around with number of layers and and neurons in each layer and different activation functions\n","* You are allowed to use nn.functional. (convolutions _might_ make your accuracy better)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O0s9pBApxUWw"},"outputs":[],"source":["#imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ck467THc_b_l"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"source":["#using the GPU if it exists\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3kNXEcEJBx9W"},"outputs":[{"data":{"text/plain":["\u003ctorch._C.Generator at 0x79c9305f21b0\u003e"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#manual seed for reproducibility\n","torch.manual_seed(1234567890)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VA--wWI__d7O"},"outputs":[],"source":["#loading and preprocessing the data\n","#i am including data augmentation and also using common standard values for mean and std\n","mean = (0.5071, 0.4867, 0.4408)\n","std = (0.2675, 0.2565, 0.2761)\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4), #adds a padding (4 px width) of zeros and crops to 32 * 32 randomly\n","    transforms.RandomHorizontalFlip(p=0.5), #flips the image horizontally with probability 0.5\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","    ])\n","\n","transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n","\n","train_dataset = datasets.CIFAR100(root=\"./data\", train=True, transform=transform_train, download=True)\n","test_dataset = datasets.CIFAR100(root=\"./data\", train=False, transform=transform_test, download=True)\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VKsm9GorKx0L"},"outputs":[{"name":"stdout","output_type":"stream","text":["training data: 50000\n","testing data: 10000\n","no of batches in training data: 391\n","no of batched in testing data: 10\n"]}],"source":["#to know the length of the datasets\n","print(f\"training data: {len(train_dataset)}\")\n","print(f\"testing data: {len(test_dataset)}\")\n","print(f\"no of batches in training data: {len(train_loader)}\")\n","print(f\"no of batched in testing data: {len(test_loader)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fpU9dToAMnxI"},"outputs":[],"source":["#Building the Model\n","SequentialNet = nn.Sequential(\n","\n","    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0), #input shape: (128, 3, 32, 32), output shape: (128, 32, 30, 30)\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2), #input shape: (128, 32, 30, 30), #output shape: (128, 32, 15, 15)\n","\n","    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, padding=2), #input shape: (128, 32, 15, 15) #output shape: (128, 64, 16, 16)\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2), #input shape: (128, 64, 16, 16), output shape: (128, 64, 8, 8)\n","\n","    nn.Flatten(), #output shape: (128, 4096)\n","    nn.Linear(4096, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, 100),\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MYNZRCCDW3nY"},"outputs":[],"source":["#training loop\n","def train(model, loader, optimizer, loss_fn, epochs):\n","  model.train()\n","  for epoch in range(epochs):\n","    total_loss = 0\n","    for x, y in loader:\n","      x, y = x.to(device), y.to(device)\n","      optimizer.zero_grad()\n","      logits = model(x)\n","      loss = loss_fn(logits, y)\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss.item()\n","    print(f\"Epoch: {epoch}, Loss: {total_loss:.4f}\")\n","\n","#testing loop\n","def test(model, loader):\n","  model.eval()\n","  correct = 0\n","  total = 0\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x, y = x.to(device), y.to(device)\n","      logits = model(x)\n","      pred = logits.argmax(dim=1)\n","      correct += (pred == y).sum().item()\n","      total += y.size(0)\n","  print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"x1Aea7W_ZUeQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training SequentialNet\n","Epoch: 0, Loss: 652.5024\n","Epoch: 1, Loss: 643.7968\n","Epoch: 2, Loss: 632.5286\n","Epoch: 3, Loss: 624.2970\n","Epoch: 4, Loss: 618.2031\n","Epoch: 5, Loss: 612.4785\n","Epoch: 6, Loss: 603.8041\n","Epoch: 7, Loss: 598.4736\n","Epoch: 8, Loss: 593.7300\n","Epoch: 9, Loss: 587.3079\n","Epoch: 10, Loss: 585.8922\n","Epoch: 11, Loss: 576.4255\n","Epoch: 12, Loss: 569.0679\n","Epoch: 13, Loss: 567.1825\n","Epoch: 14, Loss: 558.6471\n","Epoch: 15, Loss: 555.4223\n","Epoch: 16, Loss: 553.4626\n","Epoch: 17, Loss: 552.2862\n","Epoch: 18, Loss: 543.1860\n","Epoch: 19, Loss: 541.3658\n","Accuracy: 50.27%\n"]}],"source":["#training\n","print(\"\\nTraining SequentialNet\")\n","sequential_model = SequentialNet.to(device)\n","optimizer_seq = optim.Adam(sequential_model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","\n","train(sequential_model, train_loader, optimizer_seq, loss_fn, epochs=20)\n","test(sequential_model, test_loader)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOKiLU/F1xOLFEFlgdT5nAq","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}