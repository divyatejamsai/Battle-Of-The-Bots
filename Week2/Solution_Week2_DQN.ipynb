{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ca209422",
      "metadata": {
        "id": "ca209422"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import pygame\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eddc73b4",
      "metadata": {
        "id": "eddc73b4"
      },
      "outputs": [],
      "source": [
        "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, state_space, action_space):\n",
        "        super().__init__()\n",
        "        # Add your architecture parameters here\n",
        "        # You can use nn.Functional\n",
        "        # Remember that the input is of size batch_size x state_space\n",
        "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
        "        # TODO: Add code here\n",
        "        self.layer1 = nn.Linear(state_space, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, action_space)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Complete based on your implementation\n",
        "        outs1 = F.relu(self.layer1(input))\n",
        "        outs2 = F.relu(self.layer2(outs1))\n",
        "        return self.layer3(outs2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "37adca73",
      "metadata": {
        "id": "37adca73"
      },
      "outputs": [],
      "source": [
        "# While training neural networks, we split the data into batches.\n",
        "# To improve the training, we need to remove the \"correlation\" between game states\n",
        "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
        "# states at random which reduces the correlation.\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement training logic for CartPole environment here\n",
        "# Remember to use the ExperienceBuffer and a target network\n",
        "# Details can be found in the book sent in the group\n",
        "\n",
        "def train(qnet, targetnet, buffer, env, episodes=1000, batch_size=64, gamma=0.99, epsilon_max = 1, epsilon_min=0.1, epsilon_decay=200, target_update=100, lr=1e-3):\n",
        "  optimiser = torch.optim.Adam(qnet.parameters(), lr = lr)\n",
        "  loss_fn = nn.MSELoss()\n",
        "\n",
        "  epsilon = epsilon_max\n",
        "  ep_rewards = []\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    state, _= env.reset()\n",
        "    state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if np.random.uniform(0,1) < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        q_values = qnet(state)\n",
        "        action = q_values.argmax().item()\n",
        "\n",
        "      next_state, reward, done, *info = env.step(action)\n",
        "      total_reward += reward\n",
        "      next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "      buffer.push(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "\n",
        "      if len(buffer) >= batch_size:\n",
        "        states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        # Add an extra dimension to actions to match the dimensions of qnet(states)\n",
        "        actions = actions.unsqueeze(1)\n",
        "\n",
        "        current_q_values = qnet(states).gather(1, actions).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = targetnet(next_states).max(1)[0]\n",
        "            target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
        "\n",
        "        loss = loss_fn(current_q_values, target_q_values)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    ep_rewards.append(total_reward)\n",
        "    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-1. * episode / epsilon_decay)\n",
        "\n",
        "    if episode % target_update == 0:\n",
        "      targetnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "      avg_reward = np.mean(ep_rewards[-100:]) if episode > 0 else total_reward\n",
        "      print(f'Episode {episode}, Average Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}')\n",
        "\n",
        "  return ep_rewards"
      ],
      "metadata": {
        "id": "mtNQEowQO5cy"
      },
      "id": "mtNQEowQO5cy",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6493553c",
      "metadata": {
        "id": "6493553c"
      },
      "outputs": [],
      "source": [
        "def evaluate_cartpole_model(model, episodes=10, render=True):\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8643afe7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8643afe7",
        "outputId": "257936f8-62a7-42d7-c590-20946c151c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Average Reward: 19.00, Epsilon: 1.000\n",
            "Episode 100, Average Reward: 17.10, Epsilon: 0.646\n",
            "Episode 200, Average Reward: 14.95, Epsilon: 0.431\n",
            "Episode 300, Average Reward: 18.57, Epsilon: 0.301\n",
            "Episode 400, Average Reward: 25.67, Epsilon: 0.222\n",
            "Episode 500, Average Reward: 40.35, Epsilon: 0.174\n",
            "Episode 600, Average Reward: 98.37, Epsilon: 0.145\n",
            "Episode 700, Average Reward: 131.45, Epsilon: 0.127\n",
            "Episode 800, Average Reward: 173.81, Epsilon: 0.116\n",
            "Episode 900, Average Reward: 321.37, Epsilon: 0.110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[19.0,\n",
              " 13.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 17.0,\n",
              " 16.0,\n",
              " 13.0,\n",
              " 21.0,\n",
              " 39.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 15.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 22.0,\n",
              " 21.0,\n",
              " 19.0,\n",
              " 20.0,\n",
              " 12.0,\n",
              " 18.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 18.0,\n",
              " 23.0,\n",
              " 10.0,\n",
              " 32.0,\n",
              " 69.0,\n",
              " 21.0,\n",
              " 20.0,\n",
              " 31.0,\n",
              " 18.0,\n",
              " 10.0,\n",
              " 14.0,\n",
              " 20.0,\n",
              " 10.0,\n",
              " 20.0,\n",
              " 11.0,\n",
              " 15.0,\n",
              " 21.0,\n",
              " 10.0,\n",
              " 21.0,\n",
              " 9.0,\n",
              " 22.0,\n",
              " 12.0,\n",
              " 59.0,\n",
              " 16.0,\n",
              " 12.0,\n",
              " 20.0,\n",
              " 26.0,\n",
              " 16.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 26.0,\n",
              " 10.0,\n",
              " 13.0,\n",
              " 13.0,\n",
              " 15.0,\n",
              " 13.0,\n",
              " 10.0,\n",
              " 30.0,\n",
              " 9.0,\n",
              " 22.0,\n",
              " 13.0,\n",
              " 40.0,\n",
              " 9.0,\n",
              " 23.0,\n",
              " 12.0,\n",
              " 31.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 15.0,\n",
              " 14.0,\n",
              " 15.0,\n",
              " 21.0,\n",
              " 16.0,\n",
              " 19.0,\n",
              " 9.0,\n",
              " 15.0,\n",
              " 13.0,\n",
              " 18.0,\n",
              " 17.0,\n",
              " 24.0,\n",
              " 11.0,\n",
              " 22.0,\n",
              " 21.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 13.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 17.0,\n",
              " 10.0,\n",
              " 11.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 13.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 13.0,\n",
              " 32.0,\n",
              " 16.0,\n",
              " 9.0,\n",
              " 8.0,\n",
              " 14.0,\n",
              " 13.0,\n",
              " 23.0,\n",
              " 13.0,\n",
              " 9.0,\n",
              " 20.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 10.0,\n",
              " 21.0,\n",
              " 17.0,\n",
              " 16.0,\n",
              " 57.0,\n",
              " 22.0,\n",
              " 10.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 15.0,\n",
              " 13.0,\n",
              " 9.0,\n",
              " 26.0,\n",
              " 12.0,\n",
              " 8.0,\n",
              " 16.0,\n",
              " 26.0,\n",
              " 9.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 14.0,\n",
              " 15.0,\n",
              " 14.0,\n",
              " 38.0,\n",
              " 12.0,\n",
              " 19.0,\n",
              " 11.0,\n",
              " 14.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 17.0,\n",
              " 12.0,\n",
              " 14.0,\n",
              " 10.0,\n",
              " 9.0,\n",
              " 31.0,\n",
              " 14.0,\n",
              " 13.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 19.0,\n",
              " 12.0,\n",
              " 15.0,\n",
              " 20.0,\n",
              " 14.0,\n",
              " 12.0,\n",
              " 28.0,\n",
              " 16.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 10.0,\n",
              " 22.0,\n",
              " 35.0,\n",
              " 28.0,\n",
              " 17.0,\n",
              " 10.0,\n",
              " 13.0,\n",
              " 11.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 15.0,\n",
              " 12.0,\n",
              " 9.0,\n",
              " 10.0,\n",
              " 10.0,\n",
              " 22.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 15.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 15.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 15.0,\n",
              " 13.0,\n",
              " 14.0,\n",
              " 14.0,\n",
              " 9.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 40.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 35.0,\n",
              " 13.0,\n",
              " 15.0,\n",
              " 12.0,\n",
              " 13.0,\n",
              " 38.0,\n",
              " 19.0,\n",
              " 18.0,\n",
              " 11.0,\n",
              " 9.0,\n",
              " 12.0,\n",
              " 18.0,\n",
              " 21.0,\n",
              " 15.0,\n",
              " 15.0,\n",
              " 9.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 14.0,\n",
              " 12.0,\n",
              " 14.0,\n",
              " 13.0,\n",
              " 16.0,\n",
              " 12.0,\n",
              " 22.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 19.0,\n",
              " 45.0,\n",
              " 19.0,\n",
              " 56.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 18.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 13.0,\n",
              " 14.0,\n",
              " 38.0,\n",
              " 20.0,\n",
              " 10.0,\n",
              " 13.0,\n",
              " 30.0,\n",
              " 44.0,\n",
              " 10.0,\n",
              " 55.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 30.0,\n",
              " 11.0,\n",
              " 27.0,\n",
              " 10.0,\n",
              " 12.0,\n",
              " 24.0,\n",
              " 8.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 10.0,\n",
              " 11.0,\n",
              " 10.0,\n",
              " 29.0,\n",
              " 53.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 59.0,\n",
              " 14.0,\n",
              " 31.0,\n",
              " 27.0,\n",
              " 10.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 11.0,\n",
              " 9.0,\n",
              " 37.0,\n",
              " 10.0,\n",
              " 14.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 34.0,\n",
              " 13.0,\n",
              " 14.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 9.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 12.0,\n",
              " 50.0,\n",
              " 30.0,\n",
              " 14.0,\n",
              " 29.0,\n",
              " 14.0,\n",
              " 28.0,\n",
              " 11.0,\n",
              " 8.0,\n",
              " 9.0,\n",
              " 89.0,\n",
              " 28.0,\n",
              " 53.0,\n",
              " 14.0,\n",
              " 44.0,\n",
              " 46.0,\n",
              " 32.0,\n",
              " 12.0,\n",
              " 10.0,\n",
              " 27.0,\n",
              " 17.0,\n",
              " 14.0,\n",
              " 33.0,\n",
              " 46.0,\n",
              " 32.0,\n",
              " 13.0,\n",
              " 52.0,\n",
              " 29.0,\n",
              " 44.0,\n",
              " 89.0,\n",
              " 32.0,\n",
              " 34.0,\n",
              " 37.0,\n",
              " 36.0,\n",
              " 23.0,\n",
              " 14.0,\n",
              " 14.0,\n",
              " 23.0,\n",
              " 27.0,\n",
              " 12.0,\n",
              " 15.0,\n",
              " 41.0,\n",
              " 59.0,\n",
              " 27.0,\n",
              " 24.0,\n",
              " 36.0,\n",
              " 13.0,\n",
              " 29.0,\n",
              " 34.0,\n",
              " 13.0,\n",
              " 27.0,\n",
              " 37.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 9.0,\n",
              " 15.0,\n",
              " 15.0,\n",
              " 27.0,\n",
              " 11.0,\n",
              " 63.0,\n",
              " 24.0,\n",
              " 29.0,\n",
              " 27.0,\n",
              " 12.0,\n",
              " 27.0,\n",
              " 47.0,\n",
              " 15.0,\n",
              " 15.0,\n",
              " 31.0,\n",
              " 11.0,\n",
              " 22.0,\n",
              " 36.0,\n",
              " 14.0,\n",
              " 9.0,\n",
              " 38.0,\n",
              " 9.0,\n",
              " 26.0,\n",
              " 28.0,\n",
              " 14.0,\n",
              " 47.0,\n",
              " 45.0,\n",
              " 31.0,\n",
              " 10.0,\n",
              " 28.0,\n",
              " 11.0,\n",
              " 30.0,\n",
              " 14.0,\n",
              " 34.0,\n",
              " 45.0,\n",
              " 9.0,\n",
              " 29.0,\n",
              " 11.0,\n",
              " 16.0,\n",
              " 15.0,\n",
              " 10.0,\n",
              " 12.0,\n",
              " 30.0,\n",
              " 32.0,\n",
              " 19.0,\n",
              " 11.0,\n",
              " 13.0,\n",
              " 16.0,\n",
              " 27.0,\n",
              " 14.0,\n",
              " 11.0,\n",
              " 12.0,\n",
              " 27.0,\n",
              " 12.0,\n",
              " 15.0,\n",
              " 33.0,\n",
              " 161.0,\n",
              " 68.0,\n",
              " 16.0,\n",
              " 16.0,\n",
              " 19.0,\n",
              " 15.0,\n",
              " 30.0,\n",
              " 77.0,\n",
              " 53.0,\n",
              " 15.0,\n",
              " 72.0,\n",
              " 37.0,\n",
              " 31.0,\n",
              " 20.0,\n",
              " 13.0,\n",
              " 29.0,\n",
              " 13.0,\n",
              " 51.0,\n",
              " 34.0,\n",
              " 14.0,\n",
              " 24.0,\n",
              " 13.0,\n",
              " 45.0,\n",
              " 27.0,\n",
              " 47.0,\n",
              " 13.0,\n",
              " 25.0,\n",
              " 25.0,\n",
              " 141.0,\n",
              " 57.0,\n",
              " 67.0,\n",
              " 27.0,\n",
              " 64.0,\n",
              " 26.0,\n",
              " 52.0,\n",
              " 14.0,\n",
              " 12.0,\n",
              " 62.0,\n",
              " 101.0,\n",
              " 140.0,\n",
              " 24.0,\n",
              " 12.0,\n",
              " 27.0,\n",
              " 46.0,\n",
              " 23.0,\n",
              " 30.0,\n",
              " 157.0,\n",
              " 25.0,\n",
              " 47.0,\n",
              " 42.0,\n",
              " 14.0,\n",
              " 15.0,\n",
              " 30.0,\n",
              " 111.0,\n",
              " 30.0,\n",
              " 34.0,\n",
              " 55.0,\n",
              " 33.0,\n",
              " 134.0,\n",
              " 35.0,\n",
              " 15.0,\n",
              " 33.0,\n",
              " 26.0,\n",
              " 46.0,\n",
              " 61.0,\n",
              " 31.0,\n",
              " 15.0,\n",
              " 24.0,\n",
              " 24.0,\n",
              " 47.0,\n",
              " 48.0,\n",
              " 54.0,\n",
              " 71.0,\n",
              " 24.0,\n",
              " 44.0,\n",
              " 76.0,\n",
              " 13.0,\n",
              " 47.0,\n",
              " 10.0,\n",
              " 85.0,\n",
              " 14.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 32.0,\n",
              " 28.0,\n",
              " 23.0,\n",
              " 38.0,\n",
              " 23.0,\n",
              " 13.0,\n",
              " 12.0,\n",
              " 32.0,\n",
              " 42.0,\n",
              " 42.0,\n",
              " 97.0,\n",
              " 30.0,\n",
              " 13.0,\n",
              " 27.0,\n",
              " 67.0,\n",
              " 188.0,\n",
              " 212.0,\n",
              " 219.0,\n",
              " 50.0,\n",
              " 64.0,\n",
              " 164.0,\n",
              " 220.0,\n",
              " 83.0,\n",
              " 24.0,\n",
              " 60.0,\n",
              " 74.0,\n",
              " 45.0,\n",
              " 75.0,\n",
              " 195.0,\n",
              " 191.0,\n",
              " 189.0,\n",
              " 45.0,\n",
              " 24.0,\n",
              " 119.0,\n",
              " 58.0,\n",
              " 164.0,\n",
              " 25.0,\n",
              " 60.0,\n",
              " 26.0,\n",
              " 136.0,\n",
              " 26.0,\n",
              " 32.0,\n",
              " 61.0,\n",
              " 12.0,\n",
              " 304.0,\n",
              " 12.0,\n",
              " 149.0,\n",
              " 21.0,\n",
              " 15.0,\n",
              " 151.0,\n",
              " 12.0,\n",
              " 121.0,\n",
              " 26.0,\n",
              " 104.0,\n",
              " 123.0,\n",
              " 66.0,\n",
              " 58.0,\n",
              " 12.0,\n",
              " 29.0,\n",
              " 53.0,\n",
              " 49.0,\n",
              " 30.0,\n",
              " 170.0,\n",
              " 13.0,\n",
              " 63.0,\n",
              " 177.0,\n",
              " 62.0,\n",
              " 166.0,\n",
              " 58.0,\n",
              " 261.0,\n",
              " 71.0,\n",
              " 114.0,\n",
              " 196.0,\n",
              " 194.0,\n",
              " 142.0,\n",
              " 30.0,\n",
              " 191.0,\n",
              " 56.0,\n",
              " 55.0,\n",
              " 31.0,\n",
              " 89.0,\n",
              " 253.0,\n",
              " 80.0,\n",
              " 44.0,\n",
              " 75.0,\n",
              " 14.0,\n",
              " 192.0,\n",
              " 124.0,\n",
              " 12.0,\n",
              " 50.0,\n",
              " 184.0,\n",
              " 41.0,\n",
              " 93.0,\n",
              " 80.0,\n",
              " 67.0,\n",
              " 13.0,\n",
              " 24.0,\n",
              " 198.0,\n",
              " 182.0,\n",
              " 57.0,\n",
              " 137.0,\n",
              " 149.0,\n",
              " 134.0,\n",
              " 63.0,\n",
              " 50.0,\n",
              " 35.0,\n",
              " 101.0,\n",
              " 199.0,\n",
              " 152.0,\n",
              " 67.0,\n",
              " 80.0,\n",
              " 305.0,\n",
              " 29.0,\n",
              " 171.0,\n",
              " 152.0,\n",
              " 74.0,\n",
              " 146.0,\n",
              " 11.0,\n",
              " 150.0,\n",
              " 143.0,\n",
              " 148.0,\n",
              " 215.0,\n",
              " 132.0,\n",
              " 146.0,\n",
              " 171.0,\n",
              " 55.0,\n",
              " 158.0,\n",
              " 184.0,\n",
              " 126.0,\n",
              " 82.0,\n",
              " 113.0,\n",
              " 214.0,\n",
              " 88.0,\n",
              " 147.0,\n",
              " 216.0,\n",
              " 145.0,\n",
              " 100.0,\n",
              " 139.0,\n",
              " 152.0,\n",
              " 151.0,\n",
              " 153.0,\n",
              " 171.0,\n",
              " 50.0,\n",
              " 139.0,\n",
              " 28.0,\n",
              " 14.0,\n",
              " 151.0,\n",
              " 156.0,\n",
              " 15.0,\n",
              " 170.0,\n",
              " 42.0,\n",
              " 13.0,\n",
              " 149.0,\n",
              " 207.0,\n",
              " 75.0,\n",
              " 157.0,\n",
              " 160.0,\n",
              " 150.0,\n",
              " 186.0,\n",
              " 135.0,\n",
              " 155.0,\n",
              " 65.0,\n",
              " 138.0,\n",
              " 81.0,\n",
              " 85.0,\n",
              " 89.0,\n",
              " 22.0,\n",
              " 199.0,\n",
              " 169.0,\n",
              " 196.0,\n",
              " 139.0,\n",
              " 186.0,\n",
              " 164.0,\n",
              " 186.0,\n",
              " 165.0,\n",
              " 145.0,\n",
              " 141.0,\n",
              " 259.0,\n",
              " 162.0,\n",
              " 166.0,\n",
              " 139.0,\n",
              " 147.0,\n",
              " 148.0,\n",
              " 162.0,\n",
              " 157.0,\n",
              " 150.0,\n",
              " 143.0,\n",
              " 132.0,\n",
              " 122.0,\n",
              " 130.0,\n",
              " 145.0,\n",
              " 158.0,\n",
              " 145.0,\n",
              " 42.0,\n",
              " 37.0,\n",
              " 139.0,\n",
              " 119.0,\n",
              " 138.0,\n",
              " 12.0,\n",
              " 136.0,\n",
              " 123.0,\n",
              " 151.0,\n",
              " 150.0,\n",
              " 156.0,\n",
              " 82.0,\n",
              " 146.0,\n",
              " 142.0,\n",
              " 198.0,\n",
              " 140.0,\n",
              " 139.0,\n",
              " 64.0,\n",
              " 197.0,\n",
              " 154.0,\n",
              " 11.0,\n",
              " 141.0,\n",
              " 146.0,\n",
              " 14.0,\n",
              " 165.0,\n",
              " 160.0,\n",
              " 150.0,\n",
              " 147.0,\n",
              " 154.0,\n",
              " 147.0,\n",
              " 142.0,\n",
              " 150.0,\n",
              " 149.0,\n",
              " 165.0,\n",
              " 153.0,\n",
              " 159.0,\n",
              " 143.0,\n",
              " 178.0,\n",
              " 183.0,\n",
              " 152.0,\n",
              " 176.0,\n",
              " 149.0,\n",
              " 148.0,\n",
              " 160.0,\n",
              " 143.0,\n",
              " 155.0,\n",
              " 166.0,\n",
              " 151.0,\n",
              " 172.0,\n",
              " 153.0,\n",
              " 143.0,\n",
              " 161.0,\n",
              " 215.0,\n",
              " 198.0,\n",
              " 157.0,\n",
              " 182.0,\n",
              " 153.0,\n",
              " 237.0,\n",
              " 153.0,\n",
              " 151.0,\n",
              " 185.0,\n",
              " 145.0,\n",
              " 192.0,\n",
              " 169.0,\n",
              " 143.0,\n",
              " 169.0,\n",
              " 165.0,\n",
              " 176.0,\n",
              " 169.0,\n",
              " 203.0,\n",
              " 159.0,\n",
              " 157.0,\n",
              " 202.0,\n",
              " 168.0,\n",
              " 172.0,\n",
              " 154.0,\n",
              " 219.0,\n",
              " 250.0,\n",
              " 171.0,\n",
              " 165.0,\n",
              " 235.0,\n",
              " 214.0,\n",
              " 189.0,\n",
              " 210.0,\n",
              " 153.0,\n",
              " 187.0,\n",
              " 181.0,\n",
              " 163.0,\n",
              " 177.0,\n",
              " 171.0,\n",
              " 172.0,\n",
              " 180.0,\n",
              " 32.0,\n",
              " 141.0,\n",
              " 36.0,\n",
              " 176.0,\n",
              " 147.0,\n",
              " 162.0,\n",
              " 217.0,\n",
              " 170.0,\n",
              " 240.0,\n",
              " 173.0,\n",
              " 202.0,\n",
              " 176.0,\n",
              " 267.0,\n",
              " 166.0,\n",
              " 165.0,\n",
              " 170.0,\n",
              " 196.0,\n",
              " 273.0,\n",
              " 241.0,\n",
              " 169.0,\n",
              " 476.0,\n",
              " 178.0,\n",
              " 183.0,\n",
              " 267.0,\n",
              " 172.0,\n",
              " 174.0,\n",
              " 192.0,\n",
              " 166.0,\n",
              " 168.0,\n",
              " 330.0,\n",
              " 251.0,\n",
              " 331.0,\n",
              " 449.0,\n",
              " 238.0,\n",
              " 144.0,\n",
              " 363.0,\n",
              " 422.0,\n",
              " 312.0,\n",
              " 16.0,\n",
              " 293.0,\n",
              " 578.0,\n",
              " 589.0,\n",
              " 117.0,\n",
              " 336.0,\n",
              " 138.0,\n",
              " 358.0,\n",
              " 145.0,\n",
              " 118.0,\n",
              " 375.0,\n",
              " 615.0,\n",
              " 312.0,\n",
              " 236.0,\n",
              " 470.0,\n",
              " 172.0,\n",
              " 1017.0,\n",
              " 340.0,\n",
              " 535.0,\n",
              " 13.0,\n",
              " 162.0,\n",
              " 289.0,\n",
              " 65.0,\n",
              " 388.0,\n",
              " 337.0,\n",
              " 375.0,\n",
              " 281.0,\n",
              " 345.0,\n",
              " 357.0,\n",
              " 272.0,\n",
              " 286.0,\n",
              " 669.0,\n",
              " 311.0,\n",
              " 310.0,\n",
              " 321.0,\n",
              " 156.0,\n",
              " 120.0,\n",
              " 405.0,\n",
              " 298.0,\n",
              " 327.0,\n",
              " 356.0,\n",
              " 241.0,\n",
              " 441.0,\n",
              " 283.0,\n",
              " 343.0,\n",
              " 289.0,\n",
              " 274.0,\n",
              " 295.0,\n",
              " 516.0,\n",
              " 344.0,\n",
              " 363.0,\n",
              " 265.0,\n",
              " 217.0,\n",
              " 353.0,\n",
              " 370.0,\n",
              " 418.0,\n",
              " 172.0,\n",
              " 291.0,\n",
              " 243.0,\n",
              " 299.0,\n",
              " 285.0,\n",
              " 312.0,\n",
              " 206.0,\n",
              " 346.0,\n",
              " 404.0,\n",
              " 593.0,\n",
              " 75.0,\n",
              " 192.0,\n",
              " 475.0,\n",
              " 354.0,\n",
              " 329.0,\n",
              " 209.0,\n",
              " 469.0,\n",
              " 331.0,\n",
              " 255.0,\n",
              " 330.0,\n",
              " 418.0,\n",
              " 208.0,\n",
              " 29.0,\n",
              " 842.0,\n",
              " 285.0,\n",
              " 354.0,\n",
              " 256.0,\n",
              " 275.0,\n",
              " 261.0,\n",
              " 406.0,\n",
              " 226.0,\n",
              " 243.0,\n",
              " 265.0,\n",
              " 538.0,\n",
              " 306.0,\n",
              " 345.0,\n",
              " 506.0,\n",
              " 713.0,\n",
              " 414.0,\n",
              " 358.0,\n",
              " 653.0,\n",
              " 683.0,\n",
              " 239.0,\n",
              " 485.0,\n",
              " 136.0,\n",
              " 251.0,\n",
              " 460.0,\n",
              " 298.0,\n",
              " 215.0,\n",
              " 208.0,\n",
              " 424.0,\n",
              " 305.0,\n",
              " 87.0,\n",
              " 80.0,\n",
              " 275.0,\n",
              " 309.0,\n",
              " 212.0,\n",
              " 203.0,\n",
              " 166.0,\n",
              " 81.0,\n",
              " 427.0,\n",
              " 410.0,\n",
              " 411.0,\n",
              " 47.0,\n",
              " 73.0,\n",
              " 141.0,\n",
              " 425.0,\n",
              " 279.0,\n",
              " 209.0,\n",
              " 243.0,\n",
              " 333.0,\n",
              " 272.0,\n",
              " 293.0,\n",
              " 207.0,\n",
              " 277.0,\n",
              " 210.0,\n",
              " 235.0,\n",
              " 29.0,\n",
              " 325.0,\n",
              " 171.0,\n",
              " 356.0,\n",
              " 177.0,\n",
              " 28.0,\n",
              " 291.0,\n",
              " 289.0,\n",
              " 159.0,\n",
              " 529.0,\n",
              " 204.0,\n",
              " 201.0,\n",
              " 299.0,\n",
              " 206.0,\n",
              " 43.0,\n",
              " 160.0,\n",
              " 173.0,\n",
              " 264.0,\n",
              " 160.0,\n",
              " 203.0,\n",
              " 241.0,\n",
              " 206.0,\n",
              " 218.0,\n",
              " 63.0,\n",
              " 587.0,\n",
              " 476.0,\n",
              " 239.0,\n",
              " 172.0,\n",
              " 262.0,\n",
              " 582.0,\n",
              " 321.0,\n",
              " 157.0,\n",
              " 298.0,\n",
              " 165.0,\n",
              " 177.0,\n",
              " 331.0,\n",
              " 189.0,\n",
              " 160.0,\n",
              " 100.0,\n",
              " 177.0,\n",
              " 398.0,\n",
              " 349.0,\n",
              " 728.0,\n",
              " 313.0,\n",
              " 195.0,\n",
              " 229.0,\n",
              " 177.0,\n",
              " 188.0,\n",
              " 183.0,\n",
              " 224.0,\n",
              " 200.0,\n",
              " 257.0,\n",
              " 204.0,\n",
              " 318.0,\n",
              " 230.0,\n",
              " 215.0,\n",
              " 215.0]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_space = env.observation_space.shape[0]\n",
        "    action_space = env.action_space.n\n",
        "\n",
        "    qnet = DQN(state_space, action_space)\n",
        "    targetnet = DQN(state_space, action_space)\n",
        "    targetnet.load_state_dict(qnet.state_dict())\n",
        "\n",
        "    buffer = ExperienceBuffer(capacity=10000)\n",
        "train(qnet, targetnet, buffer, env)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run evaluation for cartpole here\n",
        "evaluate_cartpole_model(qnet, episodes=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQ-3bPCfEFg",
        "outputId": "885f5c3a-1d1f-469b-e212-35de0d94e2e0"
      },
      "id": "TaQ-3bPCfEFg",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward = 416.0\n",
            "Episode 2: Reward = 420.0\n",
            "Episode 3: Reward = 149.0\n",
            "Episode 4: Reward = 362.0\n",
            "Episode 5: Reward = 114.0\n",
            "Episode 6: Reward = 135.0\n",
            "Episode 7: Reward = 117.0\n",
            "Episode 8: Reward = 378.0\n",
            "Episode 9: Reward = 304.0\n",
            "Episode 10: Reward = 407.0\n",
            "Average reward over 10 episodes: 280.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b3c4213b",
      "metadata": {
        "id": "b3c4213b"
      },
      "outputs": [],
      "source": [
        "class SnakeGame(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, size=10, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.cell_size = 30\n",
        "        self.screen_size = self.size * self.cell_size\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
        "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
        "\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "\n",
        "        self.snake = deque()\n",
        "        self.food = None\n",
        "        self.direction = [1, 0]\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.snake.clear()\n",
        "        mid = self.size // 2\n",
        "        self.snake.appendleft([mid, mid])\n",
        "        self.direction = [1, 0]\n",
        "        self._place_food()\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_init()\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Change reward schema to avoid the following\n",
        "        # 1) 180 degree turns\n",
        "        # 2) Wall collisions\n",
        "        # 3) Being slow at collecting food\n",
        "        dist_before = abs(self.snake[0][0] - self.food[0]) + abs(self.snake[0][1] - self.food[1])\n",
        "\n",
        "\n",
        "        if action == 0 and self.direction != [-1, 0]: self.direction = [1, 0]\n",
        "        elif action == 1 and self.direction != [0, 1]: self.direction = [0, -1]\n",
        "        elif action == 2 and self.direction != [1, 0]: self.direction = [-1, 0]\n",
        "        elif action == 3 and self.direction != [0, -1]: self.direction = [0, 1]\n",
        "\n",
        "        head = self.snake[0]\n",
        "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
        "\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
        "            done = True\n",
        "            reward = -10\n",
        "        else:\n",
        "            body_to_check = list(self.snake)[:-1] if new_head != self.food else list(self.snake)\n",
        "            if new_head in body_to_check:\n",
        "                done = True\n",
        "                reward = -10\n",
        "\n",
        "        if not done:\n",
        "            self.snake.appendleft(new_head)\n",
        "            if new_head == self.food:\n",
        "                reward = 10\n",
        "                self._place_food()\n",
        "            else:\n",
        "                self.snake.pop()\n",
        "                dist_after = abs(self.snake[0][0] - self.food[0]) + abs(self.snake[0][1] - self.food[1])\n",
        "                if dist_after < dist_before:\n",
        "                    reward = 1\n",
        "                else:\n",
        "                    reward = -1\n",
        "\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        return obs, reward, done, False, {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Return an observation state, take inspiration from the observation_space attribute\n",
        "        obs = np.zeros((self.size, self.size), dtype=np.uint8)\n",
        "        if self.food:\n",
        "          obs[self.food[1], self.food[0]] = 2\n",
        "        for part in self.snake:\n",
        "          obs[part[1], part[0]] = 1\n",
        "        return obs\n",
        "\n",
        "    def _place_food(self):\n",
        "        positions = set(tuple(p) for p in self.snake)\n",
        "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
        "        self.food = list(random.choice(empty)) if empty else None\n",
        "\n",
        "    def render(self):\n",
        "        if self.screen is None:\n",
        "            self._render_init()\n",
        "\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        for x, y in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (0, 255, 0),\n",
        "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            pygame.draw.rect(\n",
        "                self.screen, (255, 0, 0),\n",
        "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
        "            )\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def _render_init(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen:\n",
        "            pygame.quit()\n",
        "            self.screen = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7d3ffb7a",
      "metadata": {
        "id": "7d3ffb7a"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training logic for Snake Game here\n",
        "def train_snake():\n",
        "    learning_rate = 1e-4\n",
        "    batch_size = 64\n",
        "    gamma = 0.99\n",
        "    num_episodes = 1000\n",
        "    buffer_capacity = 10000\n",
        "    target_update_freq = 15\n",
        "\n",
        "    epsilon_start = 1.0\n",
        "    epsilon_end = 0.05\n",
        "    epsilon_decay = 0.995\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = SnakeGame()\n",
        "\n",
        "    state_space_size = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
        "    action_space_size = env.action_space.n\n",
        "\n",
        "    policy_net = DQN(state_space_size, action_space_size).to(device)\n",
        "    target_net = DQN(state_space_size, action_space_size).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    replay_buffer = ExperienceBuffer(capacity=buffer_capacity)\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "    all_scores = []\n",
        "\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).flatten().unsqueeze(0)\n",
        "\n",
        "        done = False\n",
        "        episode_score = 0\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_values = policy_net(state)\n",
        "                action = q_values.argmax().item()\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode_score += reward\n",
        "\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = torch.tensor(next_state, dtype=torch.float32, device=device).flatten().unsqueeze(0)\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                optimize_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device)\n",
        "\n",
        "        all_scores.append(episode_score)\n",
        "\n",
        "        if epsilon > epsilon_end:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % target_update_freq == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_score = np.mean(all_scores[-50:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Score (Last 50): {avg_score:.2f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    torch.save(policy_net.state_dict(), 'snake_model_weights.pth')\n",
        "    print(\"Model weights saved to snake_model_weights.pth\")\n",
        "    env.close()\n",
        "\n",
        "def optimize_model(policy_net, target_net, buffer, optimizer, batch_size, gamma, device):\n",
        "    states, actions, rewards, next_states_np, dones = buffer.sample(batch_size)\n",
        "\n",
        "    states = torch.cat(states)\n",
        "    next_states_list = [torch.tensor(s, dtype=torch.float32, device=device).flatten().unsqueeze(0) for s in next_states_np]\n",
        "    next_states = torch.cat(next_states_list)\n",
        "    actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "\n",
        "    current_q_values = policy_net(states).gather(1, actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "\n",
        "    expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "    loss = F.smooth_l1_loss(current_q_values, expected_q_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_snake()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXlRHGGrcfo5",
        "outputId": "9494b3fd-337d-4fc0-9a40-315e0b53f382"
      },
      "id": "sXlRHGGrcfo5",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50/1000 | Avg Score (Last 50): -12.04 | Epsilon: 0.778\n",
            "Episode 100/1000 | Avg Score (Last 50): -10.84 | Epsilon: 0.606\n",
            "Episode 150/1000 | Avg Score (Last 50): -10.80 | Epsilon: 0.471\n",
            "Episode 200/1000 | Avg Score (Last 50): -10.40 | Epsilon: 0.367\n",
            "Episode 250/1000 | Avg Score (Last 50): -9.20 | Epsilon: 0.286\n",
            "Episode 300/1000 | Avg Score (Last 50): -9.94 | Epsilon: 0.222\n",
            "Episode 350/1000 | Avg Score (Last 50): -9.16 | Epsilon: 0.173\n",
            "Episode 400/1000 | Avg Score (Last 50): -6.60 | Epsilon: 0.135\n",
            "Episode 450/1000 | Avg Score (Last 50): -8.84 | Epsilon: 0.105\n",
            "Episode 500/1000 | Avg Score (Last 50): -8.66 | Epsilon: 0.082\n",
            "Episode 550/1000 | Avg Score (Last 50): -8.60 | Epsilon: 0.063\n",
            "Episode 600/1000 | Avg Score (Last 50): -9.34 | Epsilon: 0.050\n",
            "Episode 650/1000 | Avg Score (Last 50): -7.58 | Epsilon: 0.050\n",
            "Episode 700/1000 | Avg Score (Last 50): -9.26 | Epsilon: 0.050\n",
            "Episode 750/1000 | Avg Score (Last 50): -11.00 | Epsilon: 0.050\n",
            "Episode 800/1000 | Avg Score (Last 50): -9.74 | Epsilon: 0.050\n",
            "Episode 850/1000 | Avg Score (Last 50): -9.56 | Epsilon: 0.050\n",
            "Episode 900/1000 | Avg Score (Last 50): -10.60 | Epsilon: 0.050\n",
            "Episode 950/1000 | Avg Score (Last 50): -10.46 | Epsilon: 0.050\n",
            "Episode 1000/1000 | Avg Score (Last 50): -11.14 | Epsilon: 0.050\n",
            "Training complete.\n",
            "Model weights saved to snake_model_weights.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a0e10565",
      "metadata": {
        "id": "a0e10565"
      },
      "outputs": [],
      "source": [
        "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
        "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
        "    model.eval()\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "        max_steps = 10000\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            state = torch.tensor(obs, dtype=torch.float32).flatten().unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()\n",
        "            steps +=1\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = sum(rewards) / episodes\n",
        "\n",
        "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1a7eb581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7eb581",
        "outputId": "52a3575c-166e-43d1-82e3-523d4a48e8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded successfully.\n",
            "Episode 1: Reward = -16\n",
            "Episode 2: Reward = -5\n",
            "Episode 3: Reward = -18\n",
            "Episode 4: Reward = -14\n",
            "Episode 5: Reward = -10\n",
            "Episode 6: Reward = -2\n",
            "Episode 7: Reward = -4\n",
            "Episode 8: Reward = 3\n",
            "Episode 9: Reward = -5\n",
            "Episode 10: Reward = -12\n",
            "Average reward over 10 episodes: -8.3\n"
          ]
        }
      ],
      "source": [
        "# TODO: Run evaluation for Snake Game here\n",
        "\n",
        "GRID_SIZE = 10\n",
        "MODEL_WEIGHTS_PATH = 'snake_model_weights.pth'\n",
        "\n",
        "state_space = GRID_SIZE * GRID_SIZE\n",
        "action_space = 4\n",
        "model_to_evaluate = DQN(state_space, action_space)\n",
        "\n",
        "\n",
        "model_to_evaluate.load_state_dict(torch.load(MODEL_WEIGHTS_PATH))\n",
        "print(\"Model weights loaded successfully.\")\n",
        "\n",
        "evaluate_snake_model(\n",
        "    model=model_to_evaluate,\n",
        "    size=10,\n",
        "    episodes=10,\n",
        "    render=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4224fabe",
      "metadata": {
        "id": "4224fabe"
      },
      "outputs": [],
      "source": [
        "class ChaseEscapeEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dt = 0.1\n",
        "        self.max_speed = 0.4\n",
        "        self.agent_radius = 0.05\n",
        "        self.target_radius = 0.05\n",
        "        self.chaser_radius = 0.07\n",
        "        self.chaser_speed = 0.03\n",
        "\n",
        "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-1,\n",
        "            high=1,\n",
        "            shape=(8,),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.screen_size = 500\n",
        "        self.np_random = None\n",
        "\n",
        "        if render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
        "        while True:\n",
        "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
        "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
        "                return pos\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.agent_pos = self.sample_pos()\n",
        "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
        "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # TODO: Decide how to pass the state (don't use pixel values)\n",
        "        agent_x, agent_y = self.agent_pos\n",
        "        agent_vx, agent_vy = self.agent_vel\n",
        "        target_x, target_y = self.target_pos\n",
        "        chaser_x, chaser_y = self.chaser_pos\n",
        "        return np.array([agent_x, agent_y, agent_vx, agent_vy, target_x, target_y, chaser_x, chaser_y], dtype=np.float32)\n",
        "\n",
        "    def _get_info(self):\n",
        "        return {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # TODO: Add reward scheme\n",
        "        # 1) Try to make the agent stay within bounds\n",
        "        # 2) The agent shouldn't idle around\n",
        "        # 3) The agent should go for the reward\n",
        "        # 4) The agent should avoid the chaser\n",
        "        reward = 0.0\n",
        "        prev_dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
        "        accel = (np.array(action) - 1) * 0.1\n",
        "        self.agent_vel += accel\n",
        "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
        "\n",
        "        new_pos = self.agent_pos.copy()\n",
        "        new_pos += self.agent_vel * self.dt\n",
        "\n",
        "        if np.any(new_pos < -1) or np.any(new_pos > 1):\n",
        "            reward -= 0.1\n",
        "\n",
        "\n",
        "        self.agent_pos = np.clip(new_pos, -1, 1)\n",
        "\n",
        "        if np.linalg.norm(self.agent_vel) < 0.01:\n",
        "            reward -= 0.05\n",
        "\n",
        "        direction = self.agent_pos - self.chaser_pos\n",
        "        norm = np.linalg.norm(direction)\n",
        "        if norm > 1e-5:\n",
        "            self.chaser_pos += self.chaser_speed * direction / norm\n",
        "\n",
        "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
        "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
        "\n",
        "        if dist_to_chaser < 0.5:\n",
        "            if norm > 1e-5:\n",
        "                projected_chaser_pos = self.chaser_pos - self.chaser_speed * direction / norm\n",
        "                prev_dist_to_chaser = np.linalg.norm(self.agent_pos - projected_chaser_pos)\n",
        "            else:\n",
        "                prev_dist_to_chaser = dist_to_chaser\n",
        "\n",
        "            delta_chaser = prev_dist_to_chaser - dist_to_chaser\n",
        "            reward += delta_chaser * 0.3\n",
        "\n",
        "        delta = prev_dist_to_target - dist_to_target\n",
        "        reward += delta * 0.5\n",
        "\n",
        "\n",
        "        terminated = False\n",
        "\n",
        "        info = {}\n",
        "        if dist_to_target < self.agent_radius + self.target_radius:\n",
        "            reward += 10.0\n",
        "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
        "            info[\"target_captured\"] = True\n",
        "\n",
        "        if dist_to_target < 0.3:\n",
        "            reward += 0.1\n",
        "\n",
        "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
        "            reward -= 3.0\n",
        "            terminated = True\n",
        "            info[\"caught_by_chaser\"] = True\n",
        "\n",
        "        return self._get_obs(), reward, terminated, False, info\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode != \"human\":\n",
        "            return\n",
        "\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                self.close()\n",
        "\n",
        "        self.screen.fill((255, 255, 255))\n",
        "\n",
        "        def to_screen(p):\n",
        "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
        "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
        "            return x, y\n",
        "\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "\n",
        "    def close(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9eca1390",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eca1390",
        "outputId": "4ad7831a-8369-4432-bbfd-2253e48cd2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 000 | Reward: -3.35 | Steps: 32 | Captures: 0 | Caught: True\n",
            "Ep 001 | Reward: -5.37 | Steps: 55 | Captures: 0 | Caught: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-792679849.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.FloatTensor(states)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 002 | Reward: -3.07 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 003 | Reward: -6.71 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 004 | Reward: -2.67 | Steps: 28 | Captures: 0 | Caught: True\n",
            "Ep 005 | Reward: -1.65 | Steps: 34 | Captures: 0 | Caught: True\n",
            "Ep 006 | Reward: -7.27 | Steps: 74 | Captures: 0 | Caught: True\n",
            "Ep 007 | Reward: -1.62 | Steps: 38 | Captures: 0 | Caught: True\n",
            "Ep 008 | Reward: -3.00 | Steps: 16 | Captures: 0 | Caught: True\n",
            "Ep 009 | Reward: -3.00 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 010 | Reward: -1.07 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 011 | Reward: -2.76 | Steps: 10 | Captures: 0 | Caught: True\n",
            "Ep 012 | Reward: -2.85 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 013 | Reward: -5.44 | Steps: 40 | Captures: 0 | Caught: True\n",
            "Ep 014 | Reward: -3.40 | Steps: 45 | Captures: 0 | Caught: True\n",
            "Ep 015 | Reward: -2.13 | Steps: 14 | Captures: 0 | Caught: True\n",
            "Ep 016 | Reward: -2.85 | Steps: 25 | Captures: 0 | Caught: True\n",
            "Ep 017 | Reward: -3.09 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 018 | Reward: -4.72 | Steps: 38 | Captures: 0 | Caught: True\n",
            "Ep 019 | Reward: 6.09 | Steps: 57 | Captures: 1 | Caught: True\n",
            "Ep 020 | Reward: -6.05 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 021 | Reward: -2.82 | Steps: 24 | Captures: 0 | Caught: True\n",
            "Ep 022 | Reward: -2.85 | Steps: 13 | Captures: 0 | Caught: True\n",
            "Ep 023 | Reward: -3.48 | Steps: 38 | Captures: 0 | Caught: True\n",
            "Ep 024 | Reward: -7.77 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 025 | Reward: -6.45 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 026 | Reward: -3.38 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 027 | Reward: -4.94 | Steps: 69 | Captures: 0 | Caught: True\n",
            "Ep 028 | Reward: -6.72 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 029 | Reward: -2.57 | Steps: 30 | Captures: 0 | Caught: True\n",
            "Ep 030 | Reward: -6.01 | Steps: 98 | Captures: 0 | Caught: True\n",
            "Ep 031 | Reward: -3.41 | Steps: 32 | Captures: 0 | Caught: True\n",
            "Ep 032 | Reward: -2.58 | Steps: 85 | Captures: 0 | Caught: True\n",
            "Ep 033 | Reward: -6.28 | Steps: 67 | Captures: 0 | Caught: True\n",
            "Ep 034 | Reward: -3.60 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 035 | Reward: -2.85 | Steps: 19 | Captures: 0 | Caught: True\n",
            "Ep 036 | Reward: -2.90 | Steps: 23 | Captures: 0 | Caught: True\n",
            "Ep 037 | Reward: -2.84 | Steps: 29 | Captures: 0 | Caught: True\n",
            "Ep 038 | Reward: -3.40 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 039 | Reward: -2.45 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 040 | Reward: -7.22 | Steps: 82 | Captures: 0 | Caught: True\n",
            "Ep 041 | Reward: -7.12 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 042 | Reward: -5.29 | Steps: 96 | Captures: 0 | Caught: True\n",
            "Ep 043 | Reward: -7.40 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 044 | Reward: -5.46 | Steps: 53 | Captures: 0 | Caught: True\n",
            "Ep 045 | Reward: -3.15 | Steps: 16 | Captures: 0 | Caught: True\n",
            "Ep 046 | Reward: -3.06 | Steps: 30 | Captures: 0 | Caught: True\n",
            "Ep 047 | Reward: -6.08 | Steps: 43 | Captures: 0 | Caught: True\n",
            "Ep 048 | Reward: -6.98 | Steps: 55 | Captures: 0 | Caught: True\n",
            "Ep 049 | Reward: -5.23 | Steps: 55 | Captures: 0 | Caught: True\n",
            "Ep 050 | Reward: -5.47 | Steps: 37 | Captures: 0 | Caught: True\n",
            "Ep 051 | Reward: -6.76 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 052 | Reward: -2.71 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 053 | Reward: -4.30 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 054 | Reward: -8.07 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 055 | Reward: -8.30 | Steps: 104 | Captures: 0 | Caught: True\n",
            "Ep 056 | Reward: -4.19 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 057 | Reward: -2.86 | Steps: 24 | Captures: 0 | Caught: True\n",
            "Ep 058 | Reward: -7.98 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 059 | Reward: -4.14 | Steps: 39 | Captures: 0 | Caught: True\n",
            "Ep 060 | Reward: -2.83 | Steps: 13 | Captures: 0 | Caught: True\n",
            "Ep 061 | Reward: -2.87 | Steps: 17 | Captures: 0 | Caught: True\n",
            "Ep 062 | Reward: -3.68 | Steps: 61 | Captures: 0 | Caught: True\n",
            "Ep 063 | Reward: -3.11 | Steps: 32 | Captures: 0 | Caught: True\n",
            "Ep 064 | Reward: -6.40 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 065 | Reward: -5.02 | Steps: 57 | Captures: 0 | Caught: True\n",
            "Ep 066 | Reward: -6.15 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 067 | Reward: -3.78 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 068 | Reward: -7.62 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 069 | Reward: -4.95 | Steps: 60 | Captures: 0 | Caught: True\n",
            "Ep 070 | Reward: -2.91 | Steps: 26 | Captures: 0 | Caught: True\n",
            "Ep 071 | Reward: -4.21 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 072 | Reward: -7.03 | Steps: 61 | Captures: 0 | Caught: True\n",
            "Ep 073 | Reward: -8.60 | Steps: 104 | Captures: 0 | Caught: True\n",
            "Ep 074 | Reward: -8.92 | Steps: 81 | Captures: 0 | Caught: True\n",
            "Ep 075 | Reward: -2.77 | Steps: 19 | Captures: 0 | Caught: True\n",
            "Ep 076 | Reward: -3.08 | Steps: 27 | Captures: 0 | Caught: True\n",
            "Ep 077 | Reward: -2.28 | Steps: 87 | Captures: 0 | Caught: True\n",
            "Ep 078 | Reward: -4.77 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 079 | Reward: -10.61 | Steps: 121 | Captures: 0 | Caught: True\n",
            "Ep 080 | Reward: -2.93 | Steps: 26 | Captures: 0 | Caught: True\n",
            "Ep 081 | Reward: -2.87 | Steps: 38 | Captures: 0 | Caught: True\n",
            "Ep 082 | Reward: -2.80 | Steps: 10 | Captures: 0 | Caught: True\n",
            "Ep 083 | Reward: -5.27 | Steps: 69 | Captures: 0 | Caught: True\n",
            "Ep 084 | Reward: -8.09 | Steps: 128 | Captures: 0 | Caught: True\n",
            "Ep 085 | Reward: -4.39 | Steps: 48 | Captures: 0 | Caught: True\n",
            "Ep 086 | Reward: -2.87 | Steps: 14 | Captures: 0 | Caught: True\n",
            "Ep 087 | Reward: -2.76 | Steps: 45 | Captures: 0 | Caught: True\n",
            "Ep 088 | Reward: -3.55 | Steps: 77 | Captures: 0 | Caught: True\n",
            "Ep 089 | Reward: -2.61 | Steps: 26 | Captures: 0 | Caught: True\n",
            "Ep 090 | Reward: -3.57 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 091 | Reward: -2.99 | Steps: 27 | Captures: 0 | Caught: True\n",
            "Ep 092 | Reward: -3.25 | Steps: 84 | Captures: 0 | Caught: True\n",
            "Ep 093 | Reward: -2.95 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 094 | Reward: -3.89 | Steps: 30 | Captures: 0 | Caught: True\n",
            "Ep 095 | Reward: -2.41 | Steps: 34 | Captures: 0 | Caught: True\n",
            "Ep 096 | Reward: -6.79 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 097 | Reward: -3.07 | Steps: 16 | Captures: 0 | Caught: True\n",
            "Ep 098 | Reward: -3.15 | Steps: 36 | Captures: 0 | Caught: True\n",
            "Ep 099 | Reward: -2.97 | Steps: 12 | Captures: 0 | Caught: True\n",
            "Ep 100 | Reward: -8.83 | Steps: 103 | Captures: 0 | Caught: True\n",
            "Ep 101 | Reward: -3.70 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 102 | Reward: -10.20 | Steps: 114 | Captures: 0 | Caught: True\n",
            "Ep 103 | Reward: -2.45 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 104 | Reward: -3.45 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 105 | Reward: -4.73 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 106 | Reward: -5.76 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 107 | Reward: -2.80 | Steps: 22 | Captures: 0 | Caught: True\n",
            "Ep 108 | Reward: -3.45 | Steps: 74 | Captures: 0 | Caught: True\n",
            "Ep 109 | Reward: -7.82 | Steps: 145 | Captures: 0 | Caught: True\n",
            "Ep 110 | Reward: -2.76 | Steps: 22 | Captures: 0 | Caught: True\n",
            "Ep 111 | Reward: -5.54 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 112 | Reward: -3.84 | Steps: 33 | Captures: 0 | Caught: True\n",
            "Ep 113 | Reward: -3.28 | Steps: 29 | Captures: 0 | Caught: True\n",
            "Ep 114 | Reward: -3.92 | Steps: 53 | Captures: 0 | Caught: True\n",
            "Ep 115 | Reward: -5.16 | Steps: 49 | Captures: 0 | Caught: True\n",
            "Ep 116 | Reward: -7.76 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 117 | Reward: -6.96 | Steps: 80 | Captures: 0 | Caught: True\n",
            "Ep 118 | Reward: -2.79 | Steps: 12 | Captures: 0 | Caught: True\n",
            "Ep 119 | Reward: -4.15 | Steps: 53 | Captures: 0 | Caught: True\n",
            "Ep 120 | Reward: 8.67 | Steps: 157 | Captures: 1 | Caught: True\n",
            "Ep 121 | Reward: -8.32 | Steps: 79 | Captures: 0 | Caught: True\n",
            "Ep 122 | Reward: -13.98 | Steps: 213 | Captures: 0 | Caught: True\n",
            "Ep 123 | Reward: -4.30 | Steps: 24 | Captures: 0 | Caught: True\n",
            "Ep 124 | Reward: -13.30 | Steps: 143 | Captures: 0 | Caught: True\n",
            "Ep 125 | Reward: -2.52 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 126 | Reward: -9.01 | Steps: 101 | Captures: 0 | Caught: True\n",
            "Ep 127 | Reward: -1.43 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 128 | Reward: 8.08 | Steps: 51 | Captures: 1 | Caught: True\n",
            "Ep 129 | Reward: -8.58 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 130 | Reward: -6.55 | Steps: 82 | Captures: 0 | Caught: True\n",
            "Ep 131 | Reward: -3.03 | Steps: 20 | Captures: 0 | Caught: True\n",
            "Ep 132 | Reward: -5.23 | Steps: 34 | Captures: 0 | Caught: True\n",
            "Ep 133 | Reward: -5.89 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 134 | Reward: -2.63 | Steps: 33 | Captures: 0 | Caught: True\n",
            "Ep 135 | Reward: -5.41 | Steps: 57 | Captures: 0 | Caught: True\n",
            "Ep 136 | Reward: -6.61 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 137 | Reward: -0.17 | Steps: 112 | Captures: 0 | Caught: True\n",
            "Ep 138 | Reward: -6.99 | Steps: 113 | Captures: 0 | Caught: True\n",
            "Ep 139 | Reward: -7.77 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 140 | Reward: -1.60 | Steps: 50 | Captures: 0 | Caught: True\n",
            "Ep 141 | Reward: 6.73 | Steps: 45 | Captures: 1 | Caught: True\n",
            "Ep 142 | Reward: -4.63 | Steps: 37 | Captures: 0 | Caught: True\n",
            "Ep 143 | Reward: -7.04 | Steps: 67 | Captures: 0 | Caught: True\n",
            "Ep 144 | Reward: -2.70 | Steps: 13 | Captures: 0 | Caught: True\n",
            "Ep 145 | Reward: -4.40 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 146 | Reward: -3.61 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 147 | Reward: -2.81 | Steps: 15 | Captures: 0 | Caught: True\n",
            "Ep 148 | Reward: -3.29 | Steps: 104 | Captures: 0 | Caught: True\n",
            "Ep 149 | Reward: -3.44 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 150 | Reward: -7.10 | Steps: 84 | Captures: 0 | Caught: True\n",
            "Ep 151 | Reward: -3.06 | Steps: 25 | Captures: 0 | Caught: True\n",
            "Ep 152 | Reward: -3.60 | Steps: 180 | Captures: 0 | Caught: True\n",
            "Ep 153 | Reward: -1.64 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 154 | Reward: -10.19 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 155 | Reward: -8.73 | Steps: 74 | Captures: 0 | Caught: True\n",
            "Ep 156 | Reward: -14.96 | Steps: 393 | Captures: 0 | Caught: True\n",
            "Ep 157 | Reward: 16.18 | Steps: 438 | Captures: 3 | Caught: True\n",
            "Ep 158 | Reward: -0.38 | Steps: 205 | Captures: 1 | Caught: True\n",
            "Ep 159 | Reward: -9.10 | Steps: 84 | Captures: 0 | Caught: True\n",
            "Ep 160 | Reward: -8.21 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 161 | Reward: -7.34 | Steps: 73 | Captures: 0 | Caught: True\n",
            "Ep 162 | Reward: -20.23 | Steps: 273 | Captures: 0 | Caught: True\n",
            "Ep 163 | Reward: -3.25 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 164 | Reward: -2.61 | Steps: 34 | Captures: 0 | Caught: True\n",
            "Ep 165 | Reward: -11.98 | Steps: 255 | Captures: 0 | Caught: True\n",
            "Ep 166 | Reward: -3.34 | Steps: 78 | Captures: 0 | Caught: True\n",
            "Ep 167 | Reward: -6.93 | Steps: 98 | Captures: 0 | Caught: True\n",
            "Ep 168 | Reward: -2.49 | Steps: 50 | Captures: 0 | Caught: True\n",
            "Ep 169 | Reward: -16.34 | Steps: 261 | Captures: 0 | Caught: True\n",
            "Ep 170 | Reward: -2.77 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 171 | Reward: -6.05 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 172 | Reward: -4.78 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 173 | Reward: -7.79 | Steps: 112 | Captures: 0 | Caught: True\n",
            "Ep 174 | Reward: -3.11 | Steps: 43 | Captures: 0 | Caught: True\n",
            "Ep 175 | Reward: -7.64 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 176 | Reward: -6.34 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 177 | Reward: -8.05 | Steps: 121 | Captures: 0 | Caught: True\n",
            "Ep 178 | Reward: -6.04 | Steps: 40 | Captures: 0 | Caught: True\n",
            "Ep 179 | Reward: -4.99 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 180 | Reward: -4.55 | Steps: 45 | Captures: 0 | Caught: True\n",
            "Ep 181 | Reward: -7.17 | Steps: 89 | Captures: 0 | Caught: True\n",
            "Ep 182 | Reward: -3.02 | Steps: 13 | Captures: 0 | Caught: True\n",
            "Ep 183 | Reward: -7.27 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 184 | Reward: 4.43 | Steps: 459 | Captures: 1 | Caught: True\n",
            "Ep 185 | Reward: -12.34 | Steps: 343 | Captures: 0 | Caught: True\n",
            "Ep 186 | Reward: -2.64 | Steps: 35 | Captures: 0 | Caught: True\n",
            "Ep 187 | Reward: -1.43 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 188 | Reward: -2.55 | Steps: 25 | Captures: 0 | Caught: True\n",
            "Ep 189 | Reward: -7.24 | Steps: 80 | Captures: 0 | Caught: True\n",
            "Ep 190 | Reward: -2.68 | Steps: 17 | Captures: 0 | Caught: True\n",
            "Ep 191 | Reward: -7.56 | Steps: 63 | Captures: 0 | Caught: True\n",
            "Ep 192 | Reward: -6.26 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 193 | Reward: -3.47 | Steps: 25 | Captures: 0 | Caught: True\n",
            "Ep 194 | Reward: -5.88 | Steps: 44 | Captures: 0 | Caught: True\n",
            "Ep 195 | Reward: -5.30 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 196 | Reward: -12.69 | Steps: 257 | Captures: 0 | Caught: True\n",
            "Ep 197 | Reward: -7.79 | Steps: 83 | Captures: 0 | Caught: True\n",
            "Ep 198 | Reward: -8.72 | Steps: 117 | Captures: 0 | Caught: True\n",
            "Ep 199 | Reward: -6.44 | Steps: 82 | Captures: 0 | Caught: True\n",
            "Ep 200 | Reward: -7.97 | Steps: 108 | Captures: 0 | Caught: True\n",
            "Ep 201 | Reward: -6.68 | Steps: 80 | Captures: 0 | Caught: True\n",
            "Ep 202 | Reward: -8.02 | Steps: 130 | Captures: 0 | Caught: True\n",
            "Ep 203 | Reward: -7.51 | Steps: 70 | Captures: 0 | Caught: True\n",
            "Ep 204 | Reward: -6.99 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 205 | Reward: -10.31 | Steps: 115 | Captures: 0 | Caught: True\n",
            "Ep 206 | Reward: -2.27 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 207 | Reward: -3.81 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 208 | Reward: -7.76 | Steps: 75 | Captures: 0 | Caught: True\n",
            "Ep 209 | Reward: -7.09 | Steps: 67 | Captures: 0 | Caught: True\n",
            "Ep 210 | Reward: 14.11 | Steps: 217 | Captures: 2 | Caught: True\n",
            "Ep 211 | Reward: -6.13 | Steps: 65 | Captures: 0 | Caught: True\n",
            "Ep 212 | Reward: -17.17 | Steps: 346 | Captures: 0 | Caught: True\n",
            "Ep 213 | Reward: -3.56 | Steps: 201 | Captures: 0 | Caught: True\n",
            "Ep 214 | Reward: -6.60 | Steps: 104 | Captures: 0 | Caught: True\n",
            "Ep 215 | Reward: -8.83 | Steps: 78 | Captures: 0 | Caught: True\n",
            "Ep 216 | Reward: -9.32 | Steps: 154 | Captures: 0 | Caught: True\n",
            "Ep 217 | Reward: -6.56 | Steps: 58 | Captures: 0 | Caught: True\n",
            "Ep 218 | Reward: -7.99 | Steps: 170 | Captures: 0 | Caught: True\n",
            "Ep 219 | Reward: 8.46 | Steps: 41 | Captures: 1 | Caught: True\n",
            "Ep 220 | Reward: -7.52 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 221 | Reward: -2.30 | Steps: 16 | Captures: 0 | Caught: True\n",
            "Ep 222 | Reward: -10.99 | Steps: 110 | Captures: 0 | Caught: True\n",
            "Ep 223 | Reward: 7.74 | Steps: 61 | Captures: 1 | Caught: True\n",
            "Ep 224 | Reward: -3.24 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 225 | Reward: -8.81 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 226 | Reward: -2.55 | Steps: 46 | Captures: 0 | Caught: True\n",
            "Ep 227 | Reward: -1.46 | Steps: 69 | Captures: 0 | Caught: True\n",
            "Ep 228 | Reward: -4.56 | Steps: 63 | Captures: 0 | Caught: True\n",
            "Ep 229 | Reward: 14.84 | Steps: 237 | Captures: 2 | Caught: True\n",
            "Ep 230 | Reward: 0.41 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 231 | Reward: -1.53 | Steps: 40 | Captures: 0 | Caught: True\n",
            "Ep 232 | Reward: 11.71 | Steps: 178 | Captures: 2 | Caught: True\n",
            "Ep 233 | Reward: -7.94 | Steps: 77 | Captures: 0 | Caught: True\n",
            "Ep 234 | Reward: -6.02 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 235 | Reward: -4.56 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 236 | Reward: 2.05 | Steps: 244 | Captures: 1 | Caught: True\n",
            "Ep 237 | Reward: -12.09 | Steps: 326 | Captures: 0 | Caught: True\n",
            "Ep 238 | Reward: 1.48 | Steps: 178 | Captures: 1 | Caught: True\n",
            "Ep 239 | Reward: -6.19 | Steps: 70 | Captures: 0 | Caught: True\n",
            "Ep 240 | Reward: 6.19 | Steps: 45 | Captures: 1 | Caught: True\n",
            "Ep 241 | Reward: -11.11 | Steps: 186 | Captures: 0 | Caught: True\n",
            "Ep 242 | Reward: -6.68 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 243 | Reward: -6.89 | Steps: 67 | Captures: 0 | Caught: True\n",
            "Ep 244 | Reward: -5.07 | Steps: 73 | Captures: 0 | Caught: True\n",
            "Ep 245 | Reward: 13.38 | Steps: 275 | Captures: 2 | Caught: True\n",
            "Ep 246 | Reward: -3.27 | Steps: 31 | Captures: 0 | Caught: True\n",
            "Ep 247 | Reward: -11.76 | Steps: 112 | Captures: 0 | Caught: True\n",
            "Ep 248 | Reward: -6.11 | Steps: 83 | Captures: 0 | Caught: True\n",
            "Ep 249 | Reward: 12.09 | Steps: 306 | Captures: 2 | Caught: True\n",
            "Ep 250 | Reward: -2.72 | Steps: 16 | Captures: 0 | Caught: True\n",
            "Ep 251 | Reward: -5.99 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 252 | Reward: -1.69 | Steps: 100 | Captures: 0 | Caught: True\n",
            "Ep 253 | Reward: -10.38 | Steps: 118 | Captures: 0 | Caught: True\n",
            "Ep 254 | Reward: 4.52 | Steps: 63 | Captures: 1 | Caught: True\n",
            "Ep 255 | Reward: -4.34 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 256 | Reward: -4.51 | Steps: 62 | Captures: 0 | Caught: True\n",
            "Ep 257 | Reward: -5.36 | Steps: 61 | Captures: 0 | Caught: True\n",
            "Ep 258 | Reward: -5.28 | Steps: 50 | Captures: 0 | Caught: True\n",
            "Ep 259 | Reward: -4.24 | Steps: 131 | Captures: 0 | Caught: True\n",
            "Ep 260 | Reward: -3.45 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 261 | Reward: 1.05 | Steps: 157 | Captures: 1 | Caught: True\n",
            "Ep 262 | Reward: -7.15 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 263 | Reward: -8.46 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 264 | Reward: -8.93 | Steps: 80 | Captures: 0 | Caught: True\n",
            "Ep 265 | Reward: -9.23 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 266 | Reward: -6.58 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 267 | Reward: -10.29 | Steps: 150 | Captures: 0 | Caught: True\n",
            "Ep 268 | Reward: -7.23 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 269 | Reward: -3.66 | Steps: 104 | Captures: 0 | Caught: True\n",
            "Ep 270 | Reward: -3.88 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 271 | Reward: -1.64 | Steps: 192 | Captures: 1 | Caught: True\n",
            "Ep 272 | Reward: 0.13 | Steps: 170 | Captures: 1 | Caught: True\n",
            "Ep 273 | Reward: -7.10 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 274 | Reward: -2.22 | Steps: 101 | Captures: 0 | Caught: True\n",
            "Ep 275 | Reward: -8.03 | Steps: 60 | Captures: 0 | Caught: True\n",
            "Ep 276 | Reward: -6.23 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 277 | Reward: 3.44 | Steps: 165 | Captures: 1 | Caught: True\n",
            "Ep 278 | Reward: -10.01 | Steps: 105 | Captures: 0 | Caught: True\n",
            "Ep 279 | Reward: -6.52 | Steps: 47 | Captures: 0 | Caught: True\n",
            "Ep 280 | Reward: -4.89 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 281 | Reward: -2.75 | Steps: 17 | Captures: 0 | Caught: True\n",
            "Ep 282 | Reward: -9.33 | Steps: 117 | Captures: 0 | Caught: True\n",
            "Ep 283 | Reward: -14.72 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 284 | Reward: 0.02 | Steps: 417 | Captures: 1 | Caught: True\n",
            "Ep 285 | Reward: 48.30 | Steps: 330 | Captures: 5 | Caught: True\n",
            "Ep 286 | Reward: -7.67 | Steps: 144 | Captures: 0 | Caught: True\n",
            "Ep 287 | Reward: 8.39 | Steps: 497 | Captures: 1 | Caught: True\n",
            "Ep 288 | Reward: -3.60 | Steps: 124 | Captures: 0 | Caught: True\n",
            "Ep 289 | Reward: -6.27 | Steps: 42 | Captures: 0 | Caught: True\n",
            "Ep 290 | Reward: 7.38 | Steps: 127 | Captures: 1 | Caught: True\n",
            "Ep 291 | Reward: -2.77 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 292 | Reward: -2.74 | Steps: 12 | Captures: 0 | Caught: True\n",
            "Ep 293 | Reward: -7.40 | Steps: 92 | Captures: 0 | Caught: True\n",
            "Ep 294 | Reward: 1.06 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 295 | Reward: 15.58 | Steps: 123 | Captures: 2 | Caught: True\n",
            "Ep 296 | Reward: -14.46 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 297 | Reward: -8.50 | Steps: 140 | Captures: 0 | Caught: True\n",
            "Ep 298 | Reward: 9.13 | Steps: 35 | Captures: 1 | Caught: True\n",
            "Ep 299 | Reward: -6.94 | Steps: 132 | Captures: 0 | Caught: True\n",
            "Ep 300 | Reward: -7.53 | Steps: 79 | Captures: 0 | Caught: True\n",
            "Ep 301 | Reward: 5.72 | Steps: 53 | Captures: 1 | Caught: True\n",
            "Ep 302 | Reward: -5.74 | Steps: 151 | Captures: 0 | Caught: True\n",
            "Ep 303 | Reward: -12.07 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 304 | Reward: -9.64 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 305 | Reward: 2.36 | Steps: 192 | Captures: 1 | Caught: True\n",
            "Ep 306 | Reward: 16.66 | Steps: 157 | Captures: 2 | Caught: True\n",
            "Ep 307 | Reward: -3.17 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 308 | Reward: -7.85 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 309 | Reward: -8.13 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 310 | Reward: -5.38 | Steps: 39 | Captures: 0 | Caught: True\n",
            "Ep 311 | Reward: -1.01 | Steps: 119 | Captures: 0 | Caught: True\n",
            "Ep 312 | Reward: -16.67 | Steps: 380 | Captures: 0 | Caught: True\n",
            "Ep 313 | Reward: -7.36 | Steps: 79 | Captures: 0 | Caught: True\n",
            "Ep 314 | Reward: -8.39 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 315 | Reward: -3.44 | Steps: 338 | Captures: 1 | Caught: True\n",
            "Ep 316 | Reward: -2.86 | Steps: 18 | Captures: 0 | Caught: True\n",
            "Ep 317 | Reward: 2.96 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 318 | Reward: -5.71 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 319 | Reward: 10.68 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 320 | Reward: -4.89 | Steps: 55 | Captures: 0 | Caught: True\n",
            "Ep 321 | Reward: -5.97 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 322 | Reward: -4.09 | Steps: 107 | Captures: 0 | Caught: True\n",
            "Ep 323 | Reward: -2.16 | Steps: 186 | Captures: 0 | Caught: True\n",
            "Ep 324 | Reward: 3.70 | Steps: 217 | Captures: 1 | Caught: True\n",
            "Ep 325 | Reward: 20.71 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 326 | Reward: -3.70 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 327 | Reward: -16.53 | Steps: 278 | Captures: 0 | Caught: True\n",
            "Ep 328 | Reward: 19.25 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 329 | Reward: -5.06 | Steps: 70 | Captures: 0 | Caught: True\n",
            "Ep 330 | Reward: -5.99 | Steps: 46 | Captures: 0 | Caught: True\n",
            "Ep 331 | Reward: -6.23 | Steps: 140 | Captures: 0 | Caught: True\n",
            "Ep 332 | Reward: -3.86 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 333 | Reward: -2.22 | Steps: 70 | Captures: 0 | Caught: True\n",
            "Ep 334 | Reward: -10.38 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 335 | Reward: 7.62 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 336 | Reward: 9.76 | Steps: 232 | Captures: 1 | Caught: True\n",
            "Ep 337 | Reward: -2.50 | Steps: 125 | Captures: 0 | Caught: True\n",
            "Ep 338 | Reward: -4.96 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 339 | Reward: -3.14 | Steps: 77 | Captures: 0 | Caught: True\n",
            "Ep 340 | Reward: -12.37 | Steps: 500 | Captures: 0 | Caught: False\n",
            "Ep 341 | Reward: -7.20 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 342 | Reward: 14.21 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 343 | Reward: 4.68 | Steps: 142 | Captures: 1 | Caught: True\n",
            "Ep 344 | Reward: 21.64 | Steps: 297 | Captures: 2 | Caught: True\n",
            "Ep 345 | Reward: -2.78 | Steps: 64 | Captures: 0 | Caught: True\n",
            "Ep 346 | Reward: -5.00 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 347 | Reward: -2.97 | Steps: 23 | Captures: 0 | Caught: True\n",
            "Ep 348 | Reward: 2.09 | Steps: 182 | Captures: 1 | Caught: True\n",
            "Ep 349 | Reward: -10.61 | Steps: 108 | Captures: 0 | Caught: True\n",
            "Ep 350 | Reward: 10.63 | Steps: 327 | Captures: 2 | Caught: True\n",
            "Ep 351 | Reward: 13.10 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 352 | Reward: -4.60 | Steps: 51 | Captures: 0 | Caught: True\n",
            "Ep 353 | Reward: -5.20 | Steps: 103 | Captures: 0 | Caught: True\n",
            "Ep 354 | Reward: -2.75 | Steps: 30 | Captures: 0 | Caught: True\n",
            "Ep 355 | Reward: -3.20 | Steps: 113 | Captures: 0 | Caught: True\n",
            "Ep 356 | Reward: -10.35 | Steps: 130 | Captures: 0 | Caught: True\n",
            "Ep 357 | Reward: -2.47 | Steps: 174 | Captures: 0 | Caught: True\n",
            "Ep 358 | Reward: 17.47 | Steps: 142 | Captures: 2 | Caught: True\n",
            "Ep 359 | Reward: 18.89 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 360 | Reward: -8.23 | Steps: 69 | Captures: 0 | Caught: True\n",
            "Ep 361 | Reward: -6.15 | Steps: 59 | Captures: 0 | Caught: True\n",
            "Ep 362 | Reward: 6.75 | Steps: 147 | Captures: 1 | Caught: True\n",
            "Ep 363 | Reward: 29.15 | Steps: 121 | Captures: 3 | Caught: True\n",
            "Ep 364 | Reward: -0.69 | Steps: 148 | Captures: 0 | Caught: True\n",
            "Ep 365 | Reward: 19.47 | Steps: 97 | Captures: 2 | Caught: True\n",
            "Ep 366 | Reward: -7.68 | Steps: 130 | Captures: 0 | Caught: True\n",
            "Ep 367 | Reward: 57.80 | Steps: 500 | Captures: 6 | Caught: False\n",
            "Ep 368 | Reward: 6.29 | Steps: 208 | Captures: 1 | Caught: True\n",
            "Ep 369 | Reward: 8.48 | Steps: 39 | Captures: 1 | Caught: True\n",
            "Ep 370 | Reward: -2.57 | Steps: 107 | Captures: 0 | Caught: True\n",
            "Ep 371 | Reward: 8.22 | Steps: 291 | Captures: 1 | Caught: True\n",
            "Ep 372 | Reward: -8.98 | Steps: 69 | Captures: 0 | Caught: True\n",
            "Ep 373 | Reward: 5.30 | Steps: 62 | Captures: 1 | Caught: True\n",
            "Ep 374 | Reward: -10.35 | Steps: 270 | Captures: 0 | Caught: True\n",
            "Ep 375 | Reward: -1.88 | Steps: 32 | Captures: 0 | Caught: True\n",
            "Ep 376 | Reward: 5.76 | Steps: 211 | Captures: 1 | Caught: True\n",
            "Ep 377 | Reward: 19.35 | Steps: 111 | Captures: 2 | Caught: True\n",
            "Ep 378 | Reward: -13.59 | Steps: 254 | Captures: 0 | Caught: True\n",
            "Ep 379 | Reward: -4.42 | Steps: 108 | Captures: 0 | Caught: True\n",
            "Ep 380 | Reward: -4.62 | Steps: 63 | Captures: 0 | Caught: True\n",
            "Ep 381 | Reward: -2.53 | Steps: 173 | Captures: 0 | Caught: True\n",
            "Ep 382 | Reward: -5.08 | Steps: 66 | Captures: 0 | Caught: True\n",
            "Ep 383 | Reward: 21.03 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 384 | Reward: -0.60 | Steps: 87 | Captures: 0 | Caught: True\n",
            "Ep 385 | Reward: -3.99 | Steps: 57 | Captures: 0 | Caught: True\n",
            "Ep 386 | Reward: -4.11 | Steps: 150 | Captures: 0 | Caught: True\n",
            "Ep 387 | Reward: -4.80 | Steps: 87 | Captures: 0 | Caught: True\n",
            "Ep 388 | Reward: 22.26 | Steps: 347 | Captures: 3 | Caught: True\n",
            "Ep 389 | Reward: -2.78 | Steps: 22 | Captures: 0 | Caught: True\n",
            "Ep 390 | Reward: -10.18 | Steps: 118 | Captures: 0 | Caught: True\n",
            "Ep 391 | Reward: 7.81 | Steps: 130 | Captures: 1 | Caught: True\n",
            "Ep 392 | Reward: -8.02 | Steps: 95 | Captures: 0 | Caught: True\n",
            "Ep 393 | Reward: 23.37 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 394 | Reward: -1.93 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 395 | Reward: 14.73 | Steps: 315 | Captures: 2 | Caught: True\n",
            "Ep 396 | Reward: 19.54 | Steps: 142 | Captures: 2 | Caught: True\n",
            "Ep 397 | Reward: -5.84 | Steps: 78 | Captures: 0 | Caught: True\n",
            "Ep 398 | Reward: 10.52 | Steps: 60 | Captures: 1 | Caught: True\n",
            "Ep 399 | Reward: -3.76 | Steps: 39 | Captures: 0 | Caught: True\n",
            "Ep 400 | Reward: -3.96 | Steps: 61 | Captures: 0 | Caught: True\n",
            "Ep 401 | Reward: -5.00 | Steps: 147 | Captures: 0 | Caught: True\n",
            "Ep 402 | Reward: 19.00 | Steps: 291 | Captures: 2 | Caught: True\n",
            "Ep 403 | Reward: 33.00 | Steps: 400 | Captures: 3 | Caught: True\n",
            "Ep 404 | Reward: -2.12 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 405 | Reward: 8.41 | Steps: 37 | Captures: 1 | Caught: True\n",
            "Ep 406 | Reward: 17.32 | Steps: 337 | Captures: 2 | Caught: True\n",
            "Ep 407 | Reward: 10.25 | Steps: 107 | Captures: 1 | Caught: True\n",
            "Ep 408 | Reward: -3.07 | Steps: 61 | Captures: 0 | Caught: True\n",
            "Ep 409 | Reward: 43.52 | Steps: 325 | Captures: 4 | Caught: True\n",
            "Ep 410 | Reward: -2.09 | Steps: 109 | Captures: 0 | Caught: True\n",
            "Ep 411 | Reward: 7.70 | Steps: 141 | Captures: 1 | Caught: True\n",
            "Ep 412 | Reward: -0.17 | Steps: 190 | Captures: 1 | Caught: True\n",
            "Ep 413 | Reward: 27.28 | Steps: 250 | Captures: 3 | Caught: True\n",
            "Ep 414 | Reward: 27.75 | Steps: 208 | Captures: 3 | Caught: True\n",
            "Ep 415 | Reward: 37.34 | Steps: 325 | Captures: 4 | Caught: True\n",
            "Ep 416 | Reward: 9.05 | Steps: 41 | Captures: 1 | Caught: True\n",
            "Ep 417 | Reward: -3.39 | Steps: 52 | Captures: 0 | Caught: True\n",
            "Ep 418 | Reward: 19.75 | Steps: 100 | Captures: 2 | Caught: True\n",
            "Ep 419 | Reward: -4.31 | Steps: 35 | Captures: 0 | Caught: True\n",
            "Ep 420 | Reward: 2.10 | Steps: 241 | Captures: 1 | Caught: True\n",
            "Ep 421 | Reward: -2.52 | Steps: 24 | Captures: 0 | Caught: True\n",
            "Ep 422 | Reward: -0.77 | Steps: 28 | Captures: 0 | Caught: True\n",
            "Ep 423 | Reward: -1.76 | Steps: 109 | Captures: 0 | Caught: True\n",
            "Ep 424 | Reward: -2.92 | Steps: 31 | Captures: 0 | Caught: True\n",
            "Ep 425 | Reward: 9.66 | Steps: 390 | Captures: 1 | Caught: True\n",
            "Ep 426 | Reward: 7.03 | Steps: 118 | Captures: 1 | Caught: True\n",
            "Ep 427 | Reward: 13.86 | Steps: 406 | Captures: 2 | Caught: True\n",
            "Ep 428 | Reward: 8.56 | Steps: 34 | Captures: 1 | Caught: True\n",
            "Ep 429 | Reward: 26.93 | Steps: 500 | Captures: 3 | Caught: False\n",
            "Ep 430 | Reward: -6.31 | Steps: 121 | Captures: 0 | Caught: True\n",
            "Ep 431 | Reward: 3.83 | Steps: 338 | Captures: 1 | Caught: True\n",
            "Ep 432 | Reward: -4.41 | Steps: 161 | Captures: 0 | Caught: True\n",
            "Ep 433 | Reward: 5.93 | Steps: 500 | Captures: 2 | Caught: False\n",
            "Ep 434 | Reward: 26.95 | Steps: 366 | Captures: 3 | Caught: True\n",
            "Ep 435 | Reward: 8.42 | Steps: 47 | Captures: 1 | Caught: True\n",
            "Ep 436 | Reward: 7.86 | Steps: 78 | Captures: 1 | Caught: True\n",
            "Ep 437 | Reward: -2.81 | Steps: 24 | Captures: 0 | Caught: True\n",
            "Ep 438 | Reward: 8.39 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 439 | Reward: -0.24 | Steps: 71 | Captures: 0 | Caught: True\n",
            "Ep 440 | Reward: -2.66 | Steps: 21 | Captures: 0 | Caught: True\n",
            "Ep 441 | Reward: 14.00 | Steps: 232 | Captures: 2 | Caught: True\n",
            "Ep 442 | Reward: 7.96 | Steps: 158 | Captures: 1 | Caught: True\n",
            "Ep 443 | Reward: 7.64 | Steps: 50 | Captures: 1 | Caught: True\n",
            "Ep 444 | Reward: 7.58 | Steps: 177 | Captures: 1 | Caught: True\n",
            "Ep 445 | Reward: -5.50 | Steps: 74 | Captures: 0 | Caught: True\n",
            "Ep 446 | Reward: 6.67 | Steps: 101 | Captures: 1 | Caught: True\n",
            "Ep 447 | Reward: -5.62 | Steps: 67 | Captures: 0 | Caught: True\n",
            "Ep 448 | Reward: 3.73 | Steps: 276 | Captures: 1 | Caught: True\n",
            "Ep 449 | Reward: 35.80 | Steps: 325 | Captures: 3 | Caught: True\n",
            "Ep 450 | Reward: 20.43 | Steps: 381 | Captures: 2 | Caught: True\n",
            "Ep 451 | Reward: -2.41 | Steps: 128 | Captures: 0 | Caught: True\n",
            "Ep 452 | Reward: -2.57 | Steps: 27 | Captures: 0 | Caught: True\n",
            "Ep 453 | Reward: -8.12 | Steps: 131 | Captures: 0 | Caught: True\n",
            "Ep 454 | Reward: -7.04 | Steps: 50 | Captures: 0 | Caught: True\n",
            "Ep 455 | Reward: -6.91 | Steps: 54 | Captures: 0 | Caught: True\n",
            "Ep 456 | Reward: 15.36 | Steps: 494 | Captures: 1 | Caught: True\n",
            "Ep 457 | Reward: 32.96 | Steps: 403 | Captures: 3 | Caught: True\n",
            "Ep 458 | Reward: -2.12 | Steps: 36 | Captures: 0 | Caught: True\n",
            "Ep 459 | Reward: 33.55 | Steps: 500 | Captures: 3 | Caught: False\n",
            "Ep 460 | Reward: -3.38 | Steps: 35 | Captures: 0 | Caught: True\n",
            "Ep 461 | Reward: 16.64 | Steps: 500 | Captures: 1 | Caught: False\n",
            "Ep 462 | Reward: 8.16 | Steps: 34 | Captures: 1 | Caught: True\n",
            "Ep 463 | Reward: 8.49 | Steps: 47 | Captures: 1 | Caught: True\n",
            "Ep 464 | Reward: 45.46 | Steps: 296 | Captures: 4 | Caught: True\n",
            "Ep 465 | Reward: 9.89 | Steps: 153 | Captures: 1 | Caught: True\n",
            "Ep 466 | Reward: -6.84 | Steps: 94 | Captures: 0 | Caught: True\n",
            "Ep 467 | Reward: -5.83 | Steps: 56 | Captures: 0 | Caught: True\n",
            "Ep 468 | Reward: 30.93 | Steps: 224 | Captures: 3 | Caught: True\n",
            "Ep 469 | Reward: 28.58 | Steps: 152 | Captures: 3 | Caught: True\n",
            "Ep 470 | Reward: -3.71 | Steps: 76 | Captures: 0 | Caught: True\n",
            "Ep 471 | Reward: 32.13 | Steps: 243 | Captures: 3 | Caught: True\n",
            "Ep 472 | Reward: 9.33 | Steps: 44 | Captures: 1 | Caught: True\n",
            "Ep 473 | Reward: 134.59 | Steps: 500 | Captures: 12 | Caught: False\n",
            "Ep 474 | Reward: 14.83 | Steps: 288 | Captures: 1 | Caught: True\n",
            "Ep 475 | Reward: -0.02 | Steps: 92 | Captures: 0 | Caught: True\n",
            "Ep 476 | Reward: -8.65 | Steps: 73 | Captures: 0 | Caught: True\n",
            "Ep 477 | Reward: 73.92 | Steps: 471 | Captures: 7 | Caught: True\n",
            "Ep 478 | Reward: 4.72 | Steps: 205 | Captures: 1 | Caught: True\n",
            "Ep 479 | Reward: 3.52 | Steps: 169 | Captures: 1 | Caught: True\n",
            "Ep 480 | Reward: 41.24 | Steps: 304 | Captures: 4 | Caught: True\n",
            "Ep 481 | Reward: -3.77 | Steps: 35 | Captures: 0 | Caught: True\n",
            "Ep 482 | Reward: 18.65 | Steps: 344 | Captures: 2 | Caught: True\n",
            "Ep 483 | Reward: 51.01 | Steps: 500 | Captures: 5 | Caught: False\n",
            "Ep 484 | Reward: 48.08 | Steps: 500 | Captures: 4 | Caught: False\n",
            "Ep 485 | Reward: 43.94 | Steps: 317 | Captures: 4 | Caught: True\n",
            "Ep 486 | Reward: 6.49 | Steps: 154 | Captures: 1 | Caught: True\n",
            "Ep 487 | Reward: -2.69 | Steps: 21 | Captures: 0 | Caught: True\n",
            "Ep 488 | Reward: -2.65 | Steps: 15 | Captures: 0 | Caught: True\n",
            "Ep 489 | Reward: 79.50 | Steps: 500 | Captures: 7 | Caught: False\n",
            "Ep 490 | Reward: -1.62 | Steps: 28 | Captures: 0 | Caught: True\n",
            "Ep 491 | Reward: -3.36 | Steps: 68 | Captures: 0 | Caught: True\n",
            "Ep 492 | Reward: 60.69 | Steps: 500 | Captures: 5 | Caught: False\n",
            "Ep 493 | Reward: 21.65 | Steps: 75 | Captures: 2 | Caught: True\n",
            "Ep 494 | Reward: -3.00 | Steps: 41 | Captures: 0 | Caught: True\n",
            "Ep 495 | Reward: -6.90 | Steps: 90 | Captures: 0 | Caught: True\n",
            "Ep 496 | Reward: -1.03 | Steps: 58 | Captures: 0 | Caught: True\n",
            "Ep 497 | Reward: -3.99 | Steps: 55 | Captures: 0 | Caught: True\n",
            "Ep 498 | Reward: -5.08 | Steps: 115 | Captures: 0 | Caught: True\n",
            "Ep 499 | Reward: -1.82 | Steps: 31 | Captures: 0 | Caught: True\n",
            "Total reward in evaluation: 8.52\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train and evaluate CatMouseEnv\n",
        "def select_action(state, model, epsilon, action_space):\n",
        "    if random.random() < epsilon:\n",
        "        return [random.randint(0, 2), random.randint(0, 2)]\n",
        "    with torch.no_grad():\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        action = q_values.argmax(dim=1).item()\n",
        "        return [action // 3, action % 3]  # Convert flat index to (x,y) action\n",
        "\n",
        "env = ChaseEscapeEnv()\n",
        "dqn = DQN(state_space=8, action_space=9)  # 3x3 = 9 actions\n",
        "target_dqn = DQN(8, 9)\n",
        "target_dqn.load_state_dict(dqn.state_dict())\n",
        "optimizer = torch.optim.Adam(dqn.parameters(), lr=1e-3)\n",
        "buffer = ExperienceBuffer(capacity=10000)\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "min_epsilon = 0.05\n",
        "epsilon_decay = 0.995\n",
        "max_steps = 500\n",
        "\n",
        "for episode in range(500):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    target_captures = 0\n",
        "    caught_by_chaser = False\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        steps += 1\n",
        "        action = select_action(state, dqn, epsilon, env.action_space)\n",
        "        flat_action = action[0] * 3 + action[1]\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        buffer.push(state, flat_action, reward, next_state, terminated)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        done = terminated\n",
        "\n",
        "        # Track extra info\n",
        "        if info.get(\"target_captured\"):\n",
        "            target_captures += 1\n",
        "        if info.get(\"caught_by_chaser\"):\n",
        "            caught_by_chaser = True\n",
        "\n",
        "        # Train if enough samples\n",
        "        if len(buffer) > batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "            states = torch.FloatTensor(states)\n",
        "            actions = torch.LongTensor(actions)\n",
        "            rewards = torch.FloatTensor(rewards)\n",
        "            next_states = torch.FloatTensor(next_states)\n",
        "            dones = torch.FloatTensor(dones)\n",
        "\n",
        "            q_values = dqn(states)\n",
        "            next_q_values = target_dqn(next_states)\n",
        "\n",
        "            q_target = rewards + gamma * next_q_values.max(1)[0] * (1 - dones)\n",
        "            q_expected = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            loss = F.mse_loss(q_expected, q_target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    if epsilon > min_epsilon:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        target_dqn.load_state_dict(dqn.state_dict())\n",
        "\n",
        "    #  Clean, compact log per episode\n",
        "    print(\n",
        "        f\"Ep {episode:03d} | \"\n",
        "        f\"Reward: {total_reward:.2f} | \"\n",
        "        f\"Steps: {steps} | \"\n",
        "        f\"Captures: {target_captures} | \"\n",
        "        f\"Caught: {caught_by_chaser}\"\n",
        "    )\n",
        "\n",
        "# Evaluation phase\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = select_action(state, dqn, epsilon=0.0, action_space=env.action_space)\n",
        "    state, reward, terminated, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    done = terminated\n",
        "    pygame.time.delay(50)\n",
        "\n",
        "print(f\"Total reward in evaluation: {total_reward:.2f}\")\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}